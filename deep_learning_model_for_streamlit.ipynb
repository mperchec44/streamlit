{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b0f589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>'The Classic War of the Worlds' by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review\n",
       "0  Positive  With all this stuff going down at the moment w...\n",
       "1  Positive  'The Classic War of the Worlds' by Timothy Hin...\n",
       "2  Negative  The film starts with a manager (Nicholas Bell)...\n",
       "3  Negative  It must be assumed that those who praised this...\n",
       "4  Positive  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"MovieReview.csv\")\n",
    "display(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "df = df.drop('sentiment', axis=1)\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38c85c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PERCHECMorgan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PERCHECMorgan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PERCHECMorgan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "for pkg in [\"punkt\", \"punkt_tab\", \"stopwords\"]:\n",
    "    nltk.download(pkg, quiet=False)  # False pour voir le log; mets True si tu veux silencieux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "511f576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62442f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0fdbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuff going moment started listening music wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  stuff going moment started listening music wat...\n",
       "1  classic war worlds timothy hines entertaining ...\n",
       "2  film starts manager nicholas bell giving welco...\n",
       "3  must assumed praised film greatest filmed oper...\n",
       "4  superbly trashy wondrously unpretentious explo..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
    "\n",
    "    # remove stopword\n",
    "    mots = word_tokenize(w.strip())\n",
    "    mots = [mot for mot in mots if mot not in stop_words]\n",
    "    return ' '.join(mots).strip()\n",
    "\n",
    "df.review = df.review.apply(lambda x :preprocess_sentence(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767ce905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.75.1-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp313-cp313-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\perchecmorgan\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-win_amd64.whl (332.0 MB)\n",
      "   ---------------------------------------- 0.0/332.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 5.5/332.0 MB 29.8 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 13.4/332.0 MB 34.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 21.0/332.0 MB 35.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 28.8/332.0 MB 36.6 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 36.4/332.0 MB 37.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 42.2/332.0 MB 35.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 47.2/332.0 MB 33.8 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 52.2/332.0 MB 32.6 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 57.1/332.0 MB 31.7 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 62.7/332.0 MB 31.0 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 67.9/332.0 MB 30.5 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 73.4/332.0 MB 30.3 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 78.9/332.0 MB 30.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 84.7/332.0 MB 29.8 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 90.2/332.0 MB 29.6 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 95.9/332.0 MB 29.5 MB/s eta 0:00:08\n",
      "   ----------- --------------------------- 102.0/332.0 MB 29.5 MB/s eta 0:00:08\n",
      "   ------------ -------------------------- 106.4/332.0 MB 29.1 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 111.1/332.0 MB 28.8 MB/s eta 0:00:08\n",
      "   ------------- ------------------------- 116.1/332.0 MB 28.5 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 120.8/332.0 MB 28.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 125.8/332.0 MB 28.1 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 131.1/332.0 MB 28.0 MB/s eta 0:00:08\n",
      "   ---------------- ---------------------- 136.3/332.0 MB 27.9 MB/s eta 0:00:08\n",
      "   ---------------- ---------------------- 141.6/332.0 MB 27.8 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 147.1/332.0 MB 27.8 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 152.6/332.0 MB 27.8 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 158.1/332.0 MB 27.8 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 163.6/332.0 MB 27.7 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 168.6/332.0 MB 27.6 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 172.8/332.0 MB 27.4 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 176.9/332.0 MB 27.2 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 181.4/332.0 MB 27.1 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 185.9/332.0 MB 26.9 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 190.3/332.0 MB 26.8 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 195.3/332.0 MB 26.6 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 200.3/332.0 MB 26.6 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 205.0/332.0 MB 26.5 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 210.0/332.0 MB 26.4 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 215.0/332.0 MB 26.4 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 220.2/332.0 MB 26.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 225.4/332.0 MB 26.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 230.7/332.0 MB 26.4 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 236.2/332.0 MB 26.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 241.7/332.0 MB 26.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 247.2/332.0 MB 26.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 251.9/332.0 MB 26.3 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 256.4/332.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 261.4/332.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 266.3/332.0 MB 26.0 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 271.3/332.0 MB 25.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 276.0/332.0 MB 25.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 281.0/332.0 MB 25.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 286.3/332.0 MB 25.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 291.5/332.0 MB 25.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 296.7/332.0 MB 25.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 302.3/332.0 MB 24.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 307.8/332.0 MB 25.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 313.8/332.0 MB 25.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 319.6/332.0 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  325.6/332.0 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.4/332.0 MB 25.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/332.0 MB 25.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/332.0 MB 25.3 MB/s eta 0:00:01\n",
      "   --------------------------------------- 332.0/332.0 MB 24.0 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.75.1-cp313-cp313-win_amd64.whl (4.6 MB)\n",
      "   ---------------------------------------- 0.0/4.6 MB ? eta -:--:--\n",
      "   -------------------------------------- - 4.5/4.6 MB 22.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.6/4.6 MB 20.7 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.3-cp313-cp313-win_amd64.whl (208 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 5.0/5.5 MB 23.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 21.4 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 19.6 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 5.2/26.4 MB 24.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 10.2/26.4 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.9/26.4 MB 24.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.9/26.4 MB 24.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.2/26.4 MB 24.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 23.5 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp313-cp313-win_amd64.whl (316 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt_einsum, ml_dtypes, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "\n",
      "   -- -------------------------------------  1/16 [libclang]\n",
      "   ------------ ---------------------------  5/16 [optree]\n",
      "   -------------------- -------------------  8/16 [grpcio]\n",
      "   ---------------------- -----------------  9/16 [google_pasta]\n",
      "   ------------------------------ --------- 12/16 [absl-py]\n",
      "   -------------------------------- ------- 13/16 [tensorboard]\n",
      "   -------------------------------- ------- 13/16 [tensorboard]\n",
      "   -------------------------------- ------- 13/16 [tensorboard]\n",
      "   -------------------------------- ------- 13/16 [tensorboard]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ----------------------------------- ---- 14/16 [keras]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ------------------------------------- -- 15/16 [tensorflow]\n",
      "   ---------------------------------------- 16/16 [tensorflow]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.75.1 keras-3.11.3 libclang-18.1.1 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa4002dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10c7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863e22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sentenceToData(tokens, WINDOW_SIZE):\n",
    "    window = np.concatenate((np.arange(-WINDOW_SIZE,0),np.arange(1,WINDOW_SIZE+1)))\n",
    "    X,Y=([],[])\n",
    "    for word_index, word in enumerate(tokens) :\n",
    "        if ((word_index - WINDOW_SIZE >= 0) and (word_index + WINDOW_SIZE <= len(tokens) - 1)) :\n",
    "            X.append(word)\n",
    "            Y.append([tokens[word_index-i] for i in window])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "X, Y = ([], [])\n",
    "for review in df.review:\n",
    "    for sentence in review.split(\".\"):\n",
    "        word_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        if len(word_list) >= WINDOW_SIZE:\n",
    "            Y1, X1 = sentenceToData(word_list, WINDOW_SIZE//2)\n",
    "            X.extend(X1)\n",
    "            Y.extend(Y1)\n",
    "    \n",
    "X = np.array(X).astype(int)\n",
    "y = np.array(Y).astype(int).reshape([-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cda2914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f949f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 15ms/step - accuracy: 0.0314 - loss: 7.6517\n",
      "Epoch 2/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 19ms/step - accuracy: 0.0582 - loss: 6.9525\n",
      "Epoch 3/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 19ms/step - accuracy: 0.0754 - loss: 6.5192\n",
      "Epoch 4/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 19ms/step - accuracy: 0.0888 - loss: 6.1879\n",
      "Epoch 5/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 19ms/step - accuracy: 0.1006 - loss: 5.9178\n",
      "Epoch 6/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 18ms/step - accuracy: 0.1114 - loss: 5.6949\n",
      "Epoch 7/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 20ms/step - accuracy: 0.1210 - loss: 5.5124\n",
      "Epoch 8/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 20ms/step - accuracy: 0.1303 - loss: 5.3633\n",
      "Epoch 9/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 19ms/step - accuracy: 0.1389 - loss: 5.2416\n",
      "Epoch 10/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 19ms/step - accuracy: 0.1464 - loss: 5.1408\n",
      "Epoch 11/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 20ms/step - accuracy: 0.1538 - loss: 5.0567\n",
      "Epoch 12/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 19ms/step - accuracy: 0.1603 - loss: 4.9855\n",
      "Epoch 13/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 19ms/step - accuracy: 0.1658 - loss: 4.9249\n",
      "Epoch 14/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 18ms/step - accuracy: 0.1715 - loss: 4.8721\n",
      "Epoch 15/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 18ms/step - accuracy: 0.1760 - loss: 4.8260\n",
      "Epoch 16/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 18ms/step - accuracy: 0.1801 - loss: 4.7854\n",
      "Epoch 17/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 19ms/step - accuracy: 0.1842 - loss: 4.7493\n",
      "Epoch 18/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.1877 - loss: 4.7171\n",
      "Epoch 19/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 19ms/step - accuracy: 0.1907 - loss: 4.6879\n",
      "Epoch 20/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 19ms/step - accuracy: 0.1936 - loss: 4.6618\n",
      "Epoch 21/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 17ms/step - accuracy: 0.1964 - loss: 4.6376\n",
      "Epoch 22/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 17ms/step - accuracy: 0.1991 - loss: 4.6157\n",
      "Epoch 23/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 17ms/step - accuracy: 0.2009 - loss: 4.5956\n",
      "Epoch 24/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 17ms/step - accuracy: 0.2031 - loss: 4.5772\n",
      "Epoch 25/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 17ms/step - accuracy: 0.2050 - loss: 4.5601\n",
      "Epoch 26/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 17ms/step - accuracy: 0.2066 - loss: 4.5443\n",
      "Epoch 27/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 17ms/step - accuracy: 0.2083 - loss: 4.5297\n",
      "Epoch 28/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 17ms/step - accuracy: 0.2101 - loss: 4.5159\n",
      "Epoch 29/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.2114 - loss: 4.5033\n",
      "Epoch 30/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 17ms/step - accuracy: 0.2125 - loss: 4.4914\n",
      "Epoch 31/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 17ms/step - accuracy: 0.2140 - loss: 4.4800\n",
      "Epoch 32/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 17ms/step - accuracy: 0.2153 - loss: 4.4696\n",
      "Epoch 33/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 18ms/step - accuracy: 0.2162 - loss: 4.4594\n",
      "Epoch 34/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 14ms/step - accuracy: 0.2174 - loss: 4.4502\n",
      "Epoch 35/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 14ms/step - accuracy: 0.2183 - loss: 4.4414\n",
      "Epoch 36/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.2192 - loss: 4.4331\n",
      "Epoch 37/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.2197 - loss: 4.4256\n",
      "Epoch 38/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.2210 - loss: 4.4177\n",
      "Epoch 39/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 14ms/step - accuracy: 0.2215 - loss: 4.4108\n",
      "Epoch 40/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 14ms/step - accuracy: 0.2224 - loss: 4.4036\n",
      "Epoch 41/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 14ms/step - accuracy: 0.2230 - loss: 4.3978\n",
      "Epoch 42/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.2235 - loss: 4.3916\n",
      "Epoch 43/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 13ms/step - accuracy: 0.2244 - loss: 4.3860\n",
      "Epoch 44/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.2247 - loss: 4.3803\n",
      "Epoch 45/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 14ms/step - accuracy: 0.2253 - loss: 4.3752\n",
      "Epoch 46/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 14ms/step - accuracy: 0.2262 - loss: 4.3704\n",
      "Epoch 47/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.2265 - loss: 4.3653\n",
      "Epoch 48/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.2269 - loss: 4.3605\n",
      "Epoch 49/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.2271 - loss: 4.3564\n",
      "Epoch 50/50\n",
      "\u001b[1m12163/12163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 14ms/step - accuracy: 0.2275 - loss: 4.3520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c8d3a78050>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size = 128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8214ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"word2vec.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "st.title(\"Modèle Word2Vec\")\n",
    "\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.load_weights(\"word2vec.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.layers[0].trainable_weights[0].numpy()\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def dot_product(vec1, vec2):\n",
    "    return np.sum((vec1*vec2))\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return dot_product(vec1, vec2)/np.sqrt(dot_product(vec1, vec1)*dot_product(vec2, vec2))\n",
    "\n",
    "def find_closest(word_index, vectors, number_closest):\n",
    "    list1=[]\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if not np.array_equal(vector, query_vector):\n",
    "            dist = cosine_similarity(vector, query_vector)\n",
    "            list1.append([dist,index])\n",
    "    return np.asarray(sorted(list1,reverse=True)[:number_closest])\n",
    "\n",
    "def compare(index_word1, index_word2, index_word3, vectors, number_closest):\n",
    "    list1=[]\n",
    "    query_vector = vectors[index_word1] - vectors[index_word2] + vectors[index_word3]\n",
    "    normalizer = Normalizer()\n",
    "    query_vector =  normalizer.fit_transform([query_vector], 'l2')\n",
    "    query_vector= query_vector[0]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if not np.array_equal(vector, query_vector):\n",
    "            dist = cosine_similarity(vector, query_vector)\n",
    "            list1.append([dist,index])\n",
    "    return np.asarray(sorted(list1,reverse=True)[:number_closest])\n",
    "\n",
    "def print_closest(word, number=10):\n",
    "    index_closest_words = find_closest(word2idx[word], vectors, number)\n",
    "    for index_word in index_closest_words :\n",
    "        print(idx2word[index_word[1]],\" -- \",index_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c136a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple d'utilisation de la fonction print_closest\n",
    "print_closest('zombie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de6da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
